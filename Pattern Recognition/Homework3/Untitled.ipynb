{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iko\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: RuntimeWarning: overflow encountered in exp\n",
      "C:\\Users\\iko\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------------------+---------------------+\n",
      "| Step-size | Epoch |  Training Accuracy  |    Test Accuracy    |\n",
      "+-----------+-------+---------------------+---------------------+\n",
      "|    1000   |   4   |  0.9711723254324152 |  0.9528301886792453 |\n",
      "|     1     |   2   |  0.9711723254324152 |  0.9528301886792453 |\n",
      "|    1000   |   12  |  0.9724535554131967 |  0.9504716981132075 |\n",
      "|    100    |   2   |  0.9724535554131967 |  0.9504716981132075 |\n",
      "|    0.1    |   15  |  0.9724535554131967 |  0.9504716981132075 |\n",
      "|    0.01   |   13  |  0.9724535554131967 |  0.9504716981132075 |\n",
      "|    0.01   |   12  |  0.9711723254324152 |  0.9504716981132075 |\n",
      "|    0.01   |   9   |  0.9718129404228059 |  0.9504716981132075 |\n",
      "|     10    |   8   |  0.9730941704035875 |  0.9481132075471698 |\n",
      "|     10    |   6   |  0.968609865470852  |  0.9481132075471698 |\n",
      "|     1     |   5   |  0.968609865470852  |  0.9481132075471698 |\n",
      "|    1000   |   15  |  0.9654067905188981 |  0.9433962264150944 |\n",
      "|    1000   |   10  |  0.9673286354900704 |  0.9433962264150944 |\n",
      "|    100    |   8   |  0.9666880204996797 |  0.9433962264150944 |\n",
      "|    0.01   |   10  |  0.9673286354900704 |  0.9433962264150944 |\n",
      "|    0.01   |   5   |  0.9666880204996797 |  0.9433962264150944 |\n",
      "|   0.001   |   15  |  0.9666880204996797 |  0.9433962264150944 |\n",
      "|   0.001   |   7   |  0.9666880204996797 |  0.9433962264150944 |\n",
      "|    1000   |   13  |  0.9660474055092889 |  0.9410377358490566 |\n",
      "|    1000   |   5   |  0.9654067905188981 |  0.9410377358490566 |\n",
      "|    1000   |   2   |  0.9673286354900704 |  0.9410377358490566 |\n",
      "|    100    |   11  |  0.9718129404228059 |  0.9410377358490566 |\n",
      "|    100    |   6   |  0.9660474055092889 |  0.9410377358490566 |\n",
      "|    100    |   1   |  0.9666880204996797 |  0.9410377358490566 |\n",
      "|     10    |   13  |  0.9583600256245997 |  0.9410377358490566 |\n",
      "|     10    |   4   |  0.9705317104420244 |  0.9410377358490566 |\n",
      "|     1     |   14  |  0.9660474055092889 |  0.9410377358490566 |\n",
      "|     1     |   13  |  0.9673286354900704 |  0.9410377358490566 |\n",
      "|     1     |   8   |  0.9666880204996797 |  0.9410377358490566 |\n",
      "|     1     |   3   |  0.9698910954516335 |  0.9410377358490566 |\n",
      "|    0.1    |   4   |  0.9654067905188981 |  0.9410377358490566 |\n",
      "|    0.01   |   14  |  0.9705317104420244 |  0.9410377358490566 |\n",
      "|    0.01   |   11  |  0.9660474055092889 |  0.9410377358490566 |\n",
      "|    0.01   |   8   |  0.9660474055092889 |  0.9410377358490566 |\n",
      "|    0.01   |   2   |  0.9666880204996797 |  0.9410377358490566 |\n",
      "|    0.01   |   1   |  0.9654067905188981 |  0.9410377358490566 |\n",
      "|    1000   |   11  |  0.9673286354900704 |  0.9386792452830188 |\n",
      "|    100    |   15  |  0.9641255605381166 |  0.9386792452830188 |\n",
      "|    100    |   13  |  0.9673286354900704 |  0.9386792452830188 |\n",
      "|    100    |   10  |  0.9577194106342088 |  0.9386792452830188 |\n",
      "|     10    |   10  |  0.9673286354900704 |  0.9386792452830188 |\n",
      "|     10    |   7   |  0.9673286354900704 |  0.9386792452830188 |\n",
      "|     10    |   3   |  0.9666880204996797 |  0.9386792452830188 |\n",
      "|     1     |   15  |  0.9673286354900704 |  0.9386792452830188 |\n",
      "|     1     |   9   |  0.9590006406149904 |  0.9386792452830188 |\n",
      "|    0.1    |   13  |  0.9590006406149904 |  0.9386792452830188 |\n",
      "|    0.1    |   10  |  0.9673286354900704 |  0.9386792452830188 |\n",
      "|    0.1    |   9   |  0.968609865470852  |  0.9386792452830188 |\n",
      "|    0.01   |   15  |  0.9692504804612428 |  0.9386792452830188 |\n",
      "|   0.001   |   14  |  0.9692504804612428 |  0.9386792452830188 |\n",
      "|   0.001   |   13  |  0.9673286354900704 |  0.9386792452830188 |\n",
      "|    1000   |   14  |  0.9609224855861627 |  0.9363207547169812 |\n",
      "|    100    |   5   |  0.9634849455477258 |  0.9363207547169812 |\n",
      "|    0.1    |   12  |  0.9564381806534273 |  0.9363207547169812 |\n",
      "|    0.1    |   5   |  0.9628443305573351 |  0.9363207547169812 |\n",
      "|   0.001   |   10  |  0.9615631005765535 |  0.9363207547169812 |\n",
      "|   0.001   |   9   |  0.9634849455477258 |  0.9363207547169812 |\n",
      "|    100    |   12  |  0.9564381806534273 |  0.9339622641509434 |\n",
      "|     10    |   11  |  0.960281870595772  |  0.9339622641509434 |\n",
      "|    0.1    |   8   |  0.960281870595772  |  0.9316037735849056 |\n",
      "|    0.01   |   7   |  0.960281870595772  |  0.9316037735849056 |\n",
      "|    0.01   |   4   |  0.9538757206918642 |  0.9316037735849056 |\n",
      "|    1000   |   7   |  0.9596412556053812 |  0.9292452830188679 |\n",
      "|    0.1    |   7   |  0.9525944907110826 |  0.9245283018867925 |\n",
      "|    100    |   3   |  0.9564381806534273 |  0.9221698113207547 |\n",
      "|     1     |   7   |  0.9551569506726457 |  0.9221698113207547 |\n",
      "|    0.01   |   6   |  0.957078795643818  |  0.9221698113207547 |\n",
      "|   0.001   |   11  |  0.9525944907110826 |  0.9221698113207547 |\n",
      "|    100    |   7   |  0.9468289557975657 |  0.9198113207547169 |\n",
      "|     1     |   12  |  0.9493914157591288 |  0.9198113207547169 |\n",
      "|   0.001   |   12  |  0.9500320307495196 |  0.9198113207547169 |\n",
      "|    0.1    |   6   |  0.9461883408071748 |  0.9150943396226415 |\n",
      "|    100    |   9   |  0.9417040358744395 |  0.9127358490566038 |\n",
      "|     1     |   11  |  0.9359385009609225 |  0.9056603773584906 |\n",
      "|     10    |   9   |  0.9301729660474055 |  0.9033018867924528 |\n",
      "|   0.001   |   8   |  0.928891736066624  |  0.9033018867924528 |\n",
      "|   0.001   |   3   |  0.9282511210762332 |  0.9009433962264151 |\n",
      "|    0.1    |   11  |  0.9256886611146701 |  0.8985849056603774 |\n",
      "|   0.001   |   5   |  0.9256886611146701 |  0.8985849056603774 |\n",
      "|    100    |   14  |  0.9224855861627163 |  0.8962264150943396 |\n",
      "|     1     |   10  |  0.9109545163356823 |  0.8891509433962265 |\n",
      "|     10    |   15  |  0.9045483664317745 |  0.8867924528301887 |\n",
      "|    0.1    |   14  |  0.903267136450993  |  0.8867924528301887 |\n",
      "|    0.1    |   3   |  0.9045483664317745 |  0.8867924528301887 |\n",
      "|    1000   |   6   |  0.9013452914798207 |  0.8844339622641509 |\n",
      "|     1     |   6   |  0.8936579115951313 |  0.8797169811320755 |\n",
      "|    0.01   |   3   |  0.9064702114029468 |  0.8773584905660378 |\n",
      "|   0.001   |   6   |  0.8872517616912236 |  0.8726415094339622 |\n",
      "|   0.001   |   4   |  0.8821268417680974 |  0.8679245283018868 |\n",
      "|     10    |   12  |  0.8802049967969251 |  0.8655660377358491 |\n",
      "|    1000   |   9   |  0.8750800768737989 |  0.8608490566037735 |\n",
      "|    1000   |   3   |  0.860345932094811  |  0.8466981132075472 |\n",
      "|     1     |   4   |  0.8417680973734786 |  0.839622641509434  |\n",
      "|    100    |   4   |  0.8552210121716848 |  0.8372641509433962 |\n",
      "|     10    |   2   |  0.8539397821909033 |  0.8349056603773585 |\n",
      "|    1000   |   1   |  0.8449711723254324 |  0.8301886792452831 |\n",
      "|    1000   |   8   |  0.823190262652146  |  0.8231132075471698 |\n",
      "|     10    |   5   |  0.8321588725176169 |  0.8207547169811321 |\n",
      "|     10    |   1   |  0.820627802690583  |  0.8066037735849056 |\n",
      "|     1     |   1   |  0.8180653427290199 |  0.8042452830188679 |\n",
      "|   0.0001  |   15  |  0.7347853939782191 |  0.7570754716981132 |\n",
      "|   0.0001  |   13  |  0.6905829596412556 |  0.7193396226415094 |\n",
      "|   0.001   |   2   |  0.6688020499679692 |  0.7004716981132075 |\n",
      "|   0.0001  |   14  |  0.6585522101217168 |  0.6816037735849056 |\n",
      "|    0.1    |   1   |  0.6489429852658553 |  0.6462264150943396 |\n",
      "|   0.0001  |   12  |  0.6252402306213966 |  0.6462264150943396 |\n",
      "|     10    |   14  |  0.6463805253042921 |  0.6297169811320755 |\n",
      "|    0.1    |   2   |  0.6457399103139013 |  0.6297169811320755 |\n",
      "|   0.0001  |   11  |  0.5771941063420885 |  0.6108490566037735 |\n",
      "|   0.0001  |   10  |  0.5605381165919282 |  0.5636792452830188 |\n",
      "|   0.0001  |   9   |  0.5124919923126201 |  0.5188679245283019 |\n",
      "|   0.0001  |   8   |  0.4907110826393338 |  0.5117924528301887 |\n",
      "|   0.0001  |   7   |  0.4727738629083921 | 0.49764150943396224 |\n",
      "|   0.001   |   1   | 0.44522741832158874 |  0.4528301886792453 |\n",
      "|   0.0001  |   6   | 0.41832158872517616 |  0.4268867924528302 |\n",
      "|   0.0001  |   5   |  0.4170403587443946 |  0.4268867924528302 |\n",
      "|   0.0001  |   4   | 0.38949391415759127 | 0.41037735849056606 |\n",
      "|   0.0001  |   3   |  0.3638693145419603 | 0.38443396226415094 |\n",
      "|   0.0001  |   2   | 0.35682254964766175 | 0.37735849056603776 |\n",
      "|   0.0001  |   1   |  0.356181934657271  | 0.37735849056603776 |\n",
      "+-----------+-------+---------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "train_features = np.load('train_features.npy')\n",
    "train_labels = np.load('train_labels.npy')\n",
    "test_features = np.load('test_features.npy')\n",
    "test_labels = np.load('test_labels.npy')\n",
    "\n",
    "#table for different outputs\n",
    "table = PrettyTable(['Step-size', 'Epoch', 'Training Accuracy', 'Test Accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "#detect size\n",
    "size = len(train_features[0])\n",
    "\n",
    "for eta_factor in range(-4, 4):\n",
    "    #initialize w\n",
    "    w = np.zeros(size)\n",
    "    for epoch in range(1, 16):\n",
    "        #step size\n",
    "        eta = 10**eta_factor\n",
    "        \n",
    "        #shuffle training data\n",
    "        rand_index = np.arange(len(train_features))\n",
    "        np.random.shuffle(rand_index)\n",
    "        train_features_shuffle = train_features[rand_index]\n",
    "        train_labels_shuffle = train_labels[rand_index]\n",
    "\n",
    "        #update weight\n",
    "        for t in range(len(train_features)):\n",
    "            x_rnd = train_features_shuffle[t]\n",
    "            y_rnd = train_labels_shuffle[t]\n",
    "            w = w + y_rnd*x_rnd*(eta/(1+np.exp(y_rnd*np.dot(w,x_rnd))))\n",
    "\n",
    "        #definition of logistic function\n",
    "        logistic = lambda x: 1/(1+np.exp(-x))\n",
    "\n",
    "        #probability P(y|x)\n",
    "        prob_y_x = lambda x, y: logistic(y*np.dot(w,x))\n",
    "\n",
    "        #training accuracy\n",
    "        true_training = 0\n",
    "        false_training = 0\n",
    "\n",
    "        for i in range(len(train_features)):\n",
    "            prob = prob_y_x(train_features[i], train_labels[i])\n",
    "            if prob > 1/2:\n",
    "                true_training += 1\n",
    "            else:\n",
    "                false_training += 1\n",
    "\n",
    "        training_accuracy = true_training/(true_training+false_training)\n",
    "\n",
    "        #test accuracy\n",
    "        true_test = 0\n",
    "        false_test = 0\n",
    "\n",
    "        for i in range(len(test_features)):\n",
    "            prob = prob_y_x(test_features[i], test_labels[i])\n",
    "            if prob > 1/2:\n",
    "                true_test += 1\n",
    "            else:\n",
    "                false_test += 1\n",
    "\n",
    "        test_accuracy = true_test/(true_test+false_test)\n",
    "\n",
    "        table.add_row([eta,epoch,training_accuracy,test_accuracy])\n",
    "\n",
    "table.sortby = 'Test Accuracy'   \n",
    "table.reversesort = True \n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
